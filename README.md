This project is a real-time sign language recognition system that helps bridge communication gaps for the hearing-impaired. We built a CNN-based model using TensorFlow that processes video input from a webcam and classifies different sign language gestures with 94% accuracy. The model was trained on labeled gesture datasets and integrated with OpenCV for real-time recognition. We optimized accuracy using confusion matrix analysis and testing on real-world conditions. In the future, we plan to add multi-language support and mobile deployment for better accessibility.

